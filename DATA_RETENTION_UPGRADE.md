# üì¶ Data Retention Upgrade - 100x Capacity Increase

## üéØ What Changed

The ML system's data retention capacity has been **massively increased** from ~150MB to **~15GB** (100x).

This allows the model to learn from **years of market history** instead of just weeks.

---

## üìä New Limits

| Setting | Old Value | New Value | Impact |
|---------|-----------|-----------|--------|
| **Data Retention** | 90 days | **730 days (2 years)** | 8x more history |
| **Max Training Examples** | 50,000 | **5,000,000** | 100x more data |
| **Price History per Token** | 1,000 | **10,000** | 10x time series depth |
| **Max DB Size** | ~150 MB | **~15 GB** | 100x capacity |

---

## üöÄ Why This Matters

### 1. **Long-Term Pattern Recognition**
- Learn from multiple market cycles
- Recognize seasonal patterns
- Understand long-term user behavior

### 2. **More Robust Models**
- Train on 5M examples instead of 50K
- Better generalization
- Less overfitting to recent volatility

### 3. **Richer Feature Engineering**
- 10,000-point price history per token
- Long-term volatility metrics
- Multi-month trend analysis

### 4. **No More Data Deletion Anxiety**
- Old valuable data stays forever (2 years)
- Cleanup runs less aggressively
- Build "market memory" over time

---

## üîß Configuration

Add to `.env`:

```bash
# Data Retention (defaults shown)
POLYBOT_ML_DATA_RETENTION_DAYS=730       # Keep 2 years of training data
POLYBOT_ML_MAX_TRAINING_EXAMPLES=5000000 # Use up to 5M examples per training run
POLYBOT_ML_DATA_COLLECTION_LIMIT=50      # Track 50 markets per cycle
```

### Adjust Based on Your Needs

| Use Case | Retention Days | Max Examples | Est. DB Size |
|----------|----------------|--------------|--------------|
| **Aggressive learning** | 365 | 2,500,000 | ~7 GB |
| **Balanced (default)** | 730 | 5,000,000 | ~15 GB |
| **Maximum memory** | 1095 (3 years) | 10,000,000 | ~30 GB |

---

## üíæ Disk Space Requirements

### Estimate Your DB Growth

```python
# Rough math:
# - 120 features per example
# - ~3 KB per row (with indexes)
# - 50 markets √ó 10 outcomes √ó 144 cycles/day = 72,000 examples/day

# At 2 years retention:
# 72,000 examples/day √ó 730 days √ó 3 KB = ~15 GB

# Check your actual size:
# du -sh data/training_data.db
```

### Cleanup Schedule

Old data is automatically deleted:
- **Every 10 retrains** (roughly every 2-3 days)
- Only data older than `POLYBOT_ML_DATA_RETENTION_DAYS`
- Terminal status (labeled + outcome measured)

---

## üéì What the Model Learns From

### Before (90 days, 50K examples)
```
Examples collected: ~6,000-10,000/month
Training window: 2-3 months
Model sees: Recent volatility only
```

### After (730 days, 5M examples)
```
Examples collected: ~2M-3M/year
Training window: 2 years
Model sees: Multiple market cycles, diverse conditions, long-term patterns
```

---

## üîç Monitoring

### Check DB Size
```bash
du -sh data/training_data.db
# Expected: Grows 50-100 MB/day initially, then stabilizes at ~15 GB
```

### Check Example Counts
```python
from polyb0t.ml.data import DataCollector

collector = DataCollector("data/training_data.db")
stats = collector.get_statistics()

print(f"Total examples: {stats['total_examples']:,}")
print(f"Labeled: {stats['labeled_examples']:,}")
print(f"Training ready: {stats['training_ready']}")
```

### Check Oldest Data
```sql
sqlite3 data/training_data.db

-- Oldest training data
SELECT datetime(MIN(timestamp), 'unixepoch') as oldest_data
FROM training_data;

-- Age distribution
SELECT 
  CASE 
    WHEN (julianday('now') - julianday(timestamp, 'unixepoch')) < 30 THEN '< 1 month'
    WHEN (julianday('now') - julianday(timestamp, 'unixepoch')) < 90 THEN '1-3 months'
    WHEN (julianday('now') - julianday(timestamp, 'unixepoch')) < 180 THEN '3-6 months'
    WHEN (julianday('now') - julianday(timestamp, 'unixepoch')) < 365 THEN '6-12 months'
    ELSE '1-2 years'
  END as age_bucket,
  COUNT(*) as examples
FROM training_data
GROUP BY age_bucket;
```

---

## üõ°Ô∏è Safety Notes

### Disk Space
- Monitor free space: `df -h`
- 15 GB is tiny by modern standards (most laptops have 256+ GB)
- DB grows predictably (~50 MB/day)

### Model Training Time
- More data = longer training time
- 5M examples: ~5-15 minutes per retrain (acceptable)
- Training happens in background (doesn't block trading)

### Memory Usage
- Training loads up to 5M examples into RAM
- LightGBM is memory-efficient (~2-3 GB peak)
- Fine for any system with 8+ GB RAM

### Backups
```bash
# Backup your training data periodically
cp data/training_data.db backups/training_data_$(date +%Y%m%d).db

# Compress old backups
gzip backups/training_data_*.db
```

---

## üéØ Bottom Line

**Before:** Model had "short-term memory" (3 months max)

**After:** Model has "long-term memory" (2 years)

This is how institutional ML systems work - they learn from **everything** and retain knowledge for **years**.

Your bot now has the data infrastructure to compete with professional trading firms.

---

## üìö Related Docs

- `ML_SYSTEM_GUIDE.md` - Complete ML system documentation
- `BROAD_MARKET_LEARNING.md` - How the bot learns from 50+ markets
- `QUICK_START_ML.md` - Getting started with ML

---

**Generated:** January 2026  
**System:** PolyB0t v2.0 - Institutional-Grade ML Trading Bot

